{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03028152-e80b-4477-bf9b-9d4dc7b9133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "\n",
    "USE_WANDB = False # enable wandb, TODO: read wandb docs and learn how to use it for real \n",
    "%env WANDB_SILENT=True\n",
    "WANDB_PROJECT_NAME = \"unet-from-scratch\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd8621f-bfca-4fe7-99e0-74ace30a141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b951d7-e017-4e24-8bcb-7baafb0b4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "# https://www.tutorialexample.com/fix-python-logging-module-not-writing-to-file-python-tutorial/\n",
    "for handler in logging.root.handlers:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s',\n",
    "                   handlers=[logging.FileHandler(\"logfile.log\", mode=\"a\"), stream_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff73e73-0f80-4384-aa4c-b4b9e86374d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarvanaImageMaskingChallangeDataset(Dataset):\n",
    "    PATH_TO_IMGS = \"/home/jurica/Desktop/projekt/Pytorch-UNet/data/imgs\"\n",
    "    PATH_TO_MASKS = \"/home/jurica/Desktop/projekt/Pytorch-UNet/data/masks\"\n",
    "    IMG_EXT = \".jpg\"\n",
    "    MASK_EXT = \"_mask.gif\"\n",
    "\n",
    "    # black: background\n",
    "    # white: car\n",
    "    mask_values = [[0,0,0], [255,255,255]]\n",
    "    \n",
    "    def __init__(self):\n",
    "        assert len(os.listdir(self.PATH_TO_IMGS)) == len(os.listdir(self.PATH_TO_MASKS))\n",
    "\n",
    "        # Each car has exactly 16 images, each one taken at different angles.\n",
    "        # Each car has a unique id and images are named according to id_01.jpg, id_02.jpg … id_16.jpg.\n",
    "        self.img_ids = [os.path.splitext(img)[0] for img in os.listdir(self.PATH_TO_IMGS)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_ids[idx] + self.IMG_EXT\n",
    "        img_path = os.path.join(self.PATH_TO_IMGS, img_name)\n",
    "\n",
    "        mask_name = self.img_ids[idx] + self.MASK_EXT\n",
    "        mask_path = os.path.join(self.PATH_TO_MASKS, mask_name)\n",
    "        \n",
    "        img_raw = Image.open(img_path)\n",
    "        mask_raw = Image.open(mask_path)\n",
    "        \n",
    "        img, mask = self.preprocess(img_raw, mask_raw)\n",
    "        \n",
    "        return {\"image\": img, \"mask\": mask}\n",
    "\n",
    "    @classmethod\n",
    "    def get_mask_values(cls):\n",
    "        return cls.mask_values\n",
    "\n",
    "    def preprocess(self, img, mask):\n",
    "        img = np.asarray(img.resize((572,572), resample=Image.Resampling.BICUBIC))\n",
    "        # convert to RGB for black&white mask\n",
    "        mask = np.asarray(mask.resize((572,572), resample=Image.Resampling.BICUBIC).convert(\"RGB\")) \n",
    "        \n",
    "        # create ground truth for cross entropy loss\n",
    "        mask_with_classes = np.zeros((572, 572), dtype=np.int64)\n",
    "        for c, mask_class_pixel_value in enumerate(self.mask_values):\n",
    "            # (img == mask_class_pixel_value) creates new np array, (img==mask_class_pixel_value).shape == img.shape\n",
    "            # (img==mask_class_pixel_value)[i] -> [True, False, True], \n",
    "            # (comparing corresponding pixel components with corresponding mask_class_pixel_value components)\n",
    "            # (img==mask_class_pixel_value).all(-1) creates new np array\n",
    "            # x.all(-1) checks if every element of x along -1 (last) dimension is True\n",
    "            # (img==mask_class_pixel_value).all(-1).shape == (H, W)\n",
    "            # in this case, check which (img==mask_class_pixel_value)[i] is equal to [True, True, True] (pixel matches class mask_class_pixel_value)\n",
    "            \n",
    "            # set pixel class to be c for every pixel in original mask image\n",
    "            # if pixel values along all channels (-1) correspondingly match values of class c\n",
    "            mask_with_classes[(mask == mask_class_pixel_value).all(-1)] = c\n",
    "        \n",
    "        # swap color axis\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        # normalize pixels?\n",
    "        img = img / 255.0\n",
    "\n",
    "        img_tensor = torch.tensor(img.copy()).to(dtype=torch.float32)\n",
    "        \n",
    "        mask_tensor = torch.tensor(mask_with_classes.copy()).to(dtype=torch.long) # Cross entropy loss requires target to be LongTensor\n",
    "        \n",
    "        return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1767549-07ea-4598-90d0-bd22a2ca4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_volume_channels, output_volume_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(input_volume_channels, output_volume_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(output_volume_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(output_volume_channels, output_volume_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(output_volume_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_volume_channels, output_volume_channels):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(input_volume_channels, output_volume_channels, kernel_size=2, stride=2)\n",
    "        self.double_conv = EncoderBlock(input_volume_channels, output_volume_channels)\n",
    "    \n",
    "    def forward(self, x, encoder_feature_map):\n",
    "        deconv_ret = self.deconv(x)\n",
    "\n",
    "        diffInHeight = encoder_feature_map.size()[2] - deconv_ret.size()[2]\n",
    "        diffInWidth = encoder_feature_map.size()[3] - deconv_ret.size()[3]\n",
    "        \n",
    "        # [padding_left, padding_right, padding_top, padding_bottom]\n",
    "        # now deconv_ret.shape == encoder_feature_map.shape\n",
    "        deconv_ret = nn.functional.pad(deconv_ret, [diffInWidth // 2, diffInWidth - diffInWidth // 2,\n",
    "                        diffInHeight // 2, diffInHeight - diffInHeight // 2])\n",
    "\n",
    "        # concat along \"channel\" dimension, dim=1 because dim=0 -> N (batch size) \n",
    "        concat_x = torch.cat([encoder_feature_map, deconv_ret], dim=1)\n",
    "\n",
    "        return self.double_conv(concat_x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc_block1 = EncoderBlock(3, 64)      \n",
    "        self.enc_block2 = EncoderBlock(64, 128)\n",
    "        self.enc_block3 = EncoderBlock(128, 256)\n",
    "        self.enc_block4 = EncoderBlock(256, 512)\n",
    "        self.enc_block5 = EncoderBlock(512, 1024)\n",
    "\n",
    "        self.dec_block1 = DecoderBlock(1024, 512)\n",
    "        self.dec_block2 = DecoderBlock(512, 256)\n",
    "        self.dec_block3 = DecoderBlock(256, 128)\n",
    "        self.dec_block4 = DecoderBlock(128, 64)\n",
    "\n",
    "        self.final_output = nn.Conv2d(64, 2, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        enc_out1 = self.enc_block1(x)\n",
    "        pooled_enc_out1 = self.max_pool(enc_out1)\n",
    "        \n",
    "        enc_out2 = self.enc_block2(pooled_enc_out1)\n",
    "        pooled_enc_out2 = self.max_pool(enc_out2)\n",
    "        \n",
    "        enc_out3 = self.enc_block3(pooled_enc_out2)\n",
    "        pooled_enc_out3 = self.max_pool(enc_out3)\n",
    "        \n",
    "        enc_out4 = self.enc_block4(pooled_enc_out3)\n",
    "        pooled_enc_out4 = self.max_pool(enc_out4)\n",
    "        \n",
    "        enc_out5 = self.enc_block5(pooled_enc_out4)\n",
    "\n",
    "        dec_out1 = self.dec_block1(enc_out5, enc_out4)\n",
    "        dec_out2 = self.dec_block2(dec_out1, enc_out3)\n",
    "        dec_out3 = self.dec_block3(dec_out2, enc_out2)\n",
    "        dec_out4 = self.dec_block4(dec_out3, enc_out1)\n",
    "\n",
    "        return nn.functional.interpolate(self.final_output(dec_out4), size=572, mode=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174a386f-a76e-46e7-a760-9e23d7efb987",
   "metadata": {},
   "outputs": [],
   "source": [
    " def iou(masks_pred, masks):\n",
    "    # resulting tensor of shape (N, masks_predH, masks_predW)\n",
    "    # each element of resulting tensor is index of value that is max at given position across channel dimension\n",
    "    # e.g., if we have 2 channels (2 classes) then values of resulting tensor are in range [0,1]\n",
    "    most_certain_pred_values = masks_pred.argmax(dim=1)\n",
    "    \n",
    "    # one_hot appends number of classes as last dimension\n",
    "    # tensor of shape (N, H, W) becomes (N, H, W, n_classes), have to correct it to (N, C, H, W)\n",
    "    masks_pred = nn.functional.one_hot(most_certain_pred_values, len(CarvanaImageMaskingChallangeDataset.get_mask_values())).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    masks = nn.functional.one_hot(masks, len(CarvanaImageMaskingChallangeDataset.get_mask_values())).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "    _iou = MulticlassJaccardIndex(num_classes=2, average=\"macro\").to(device=device) # Calculate statistics for each label and average them\n",
    "    return _iou(masks_pred, masks).item()\n",
    "\n",
    "def train_model(model, device, epochs, batch_size, learning_rate, weight_decay, momentum, gradient_clipping, val_percent):\n",
    "    dataset = CarvanaImageMaskingChallangeDataset()\n",
    "\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], torch.Generator().manual_seed(1337))\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=False, drop_last=True)\n",
    "\n",
    "    #optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, foreach=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay, fused=True)\n",
    "    learning_rate = 0.001 # default Adam learning rate\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5) # maximize IOU\n",
    "    criterion = nn.CrossEntropyLoss() # default reduction: the weighted mean of the output of LogSoftmax is taken\n",
    "\n",
    "\n",
    "    if USE_WANDB:\n",
    "        experiment = wandb.init(project=WANDB_PROJECT_NAME)\n",
    "        experiment.config.update(\n",
    "        dict(epochs=epochs, \n",
    "             batch_size=batch_size,\n",
    "             \n",
    "             n_train=n_train,\n",
    "             n_val=n_val,\n",
    "             \n",
    "             learning_rate=learning_rate,\n",
    "             weight_decay=weight_decay,\n",
    "             momentum=momentum,\n",
    "             gradient_clipping=gradient_clipping,\n",
    "             \n",
    "             val_percent=val_percent)\n",
    "        )\n",
    "\n",
    "    logging.info(\n",
    "        f\"\"\"Starting training:\n",
    "        Epochs:              {epochs}\n",
    "        Batch size:          {batch_size}\n",
    "\n",
    "        Training set size:   {n_train}\n",
    "        Validation set size: {n_val}\n",
    "        \n",
    "        Learning rate:       {learning_rate}\n",
    "        Weight decay:        {weight_decay}\n",
    "        Momentum:            {momentum}\n",
    "        Gradient clipping:   {gradient_clipping}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # During training, the global_step is updated every time a batch is processed\n",
    "    # When logging training metrics to wandb, the global_step is used as the x-axis to indicate this\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train() # for BatchNorm\n",
    "\n",
    "        with tqdm(total=n_train, desc=f\"Epoch {epoch}/{epochs}\", unit=\"img\") as progress:\n",
    "            for batch in train_loader:\n",
    "                images = batch[\"image\"].to(device=device, memory_format=torch.channels_last)\n",
    "                masks = batch[\"mask\"].to(device=device)\n",
    "\n",
    "                masks_pred = model(images)\n",
    "\n",
    "                loss = criterion(masks_pred, masks)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping) # can't hurt?\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                # update number of remaining images\n",
    "                progress.update(images.shape[0])\n",
    "                progress.set_postfix(**{\"loss (batch)\" : loss.item()})\n",
    "                global_step += 1\n",
    "\n",
    "                if USE_WANDB:\n",
    "                    experiment.log({\n",
    "                    \"train loss\": loss.item(),\n",
    "                    \"step\": global_step,\n",
    "                    \"epoch\": epoch\n",
    "                    })\n",
    "                \n",
    "                division_step = (n_train // (5 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        if USE_WANDB:\n",
    "                            histograms = {}\n",
    "                            for tag, value in model.named_parameters():\n",
    "                                tag = tag.replace('/', '.')\n",
    "                                if not (torch.isinf(value) | torch.isnan(value)).any():\n",
    "                                    histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                                if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
    "                                    histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "                        \n",
    "                        val_score = 0\n",
    "                        with torch.inference_mode(): # disable gradient calculation and other vudu\n",
    "                            model.eval() # for BatchNorm\n",
    "                            \n",
    "                            iou_score = 0\n",
    "    \n",
    "                            # leave=True? Position=0?\n",
    "                            for batch in tqdm(val_loader, total=n_val, desc=\"Validation\", unit=\"batch\", position=0, leave=True):\n",
    "                                images = batch[\"image\"].to(device=device, memory_format=torch.channels_last)\n",
    "                                masks = batch[\"mask\"].to(device=device)\n",
    "    \n",
    "                                masks_pred = model(images)\n",
    "\n",
    "                                iou_score += iou(masks_pred, masks)\n",
    "                            \n",
    "                            model.train()\n",
    "                            val_score = iou_score / max(n_val, 1)\n",
    "                            logging.info(f\"IOU score: {val_score}\")\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        if USE_WANDB:\n",
    "                            experiment.log({\n",
    "                                'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                                'validation IOU': iou_score,\n",
    "                                'step': global_step,\n",
    "                                'epoch': epoch,\n",
    "                                **histograms\n",
    "                            })                     \n",
    "        \n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, \"/home/jurica/Desktop/projekt/pokusaj/checkpoints/checkpoint_epoch{}.pth\".format(epoch))\n",
    "        logging.info(f\"Checkpoint {epoch} saved!\")\n",
    "\n",
    "    if USE_WANDB:\n",
    "        experiment.finish()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9068110-a928-4caf-9f00-54a692289d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting training:\n",
      "       Epochs:              3\n",
      "       Batch size:          1\n",
      "\n",
      "       Training set size:   4565\n",
      "       Validation set size: 507\n",
      "       \n",
      "       Learning rate:       0.001\n",
      "       Weight decay:        1e-08\n",
      "       Momentum:            0.999\n",
      "       Gradient clipping:   1.0\n",
      "       \n",
      "Validation: 100%|██████████████████████████| 507/507 [00:39<00:00, 12.81batch/s]\n",
      "INFO: IOU score: 0.8932622394853325\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:36<00:00, 13.82batch/s]\n",
      "INFO: IOU score: 0.9797208322105558\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:36<00:00, 13.85batch/s]\n",
      "INFO: IOU score: 0.9259961474341518\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:39<00:00, 12.99batch/s]\n",
      "INFO: IOU score: 0.9039142315204327\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:38<00:00, 13.29batch/s]\n",
      "INFO: IOU score: 0.9328345683434541\n",
      "Epoch 1/3: 100%|██████| 4565/4565 [20:56<00:00,  3.63img/s, loss (batch)=0.0158]\n",
      "INFO: Checkpoint 1 saved!\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:37<00:00, 13.65batch/s]\n",
      "INFO: IOU score: 0.9824855978907447\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:36<00:00, 13.72batch/s]\n",
      "INFO: IOU score: 0.9675655434352641\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:36<00:00, 13.72batch/s]\n",
      "INFO: IOU score: 0.9846765427194404\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.69batch/s]\n",
      "INFO: IOU score: 0.984467105282364\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.67batch/s]\n",
      "INFO: IOU score: 0.9873465162057143\n",
      "Epoch 2/3: 100%|█████| 4565/4565 [20:38<00:00,  3.69img/s, loss (batch)=0.00845]\n",
      "INFO: Checkpoint 2 saved!\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.66batch/s]\n",
      "INFO: IOU score: 0.9787254426371181\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.69batch/s]\n",
      "INFO: IOU score: 0.9815683022758664\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.69batch/s]\n",
      "INFO: IOU score: 0.978822976409566\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.49batch/s]\n",
      "INFO: IOU score: 0.9396538723855329\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:36<00:00, 13.71batch/s]\n",
      "INFO: IOU score: 0.9820751122233901\n",
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4565/4565 [20:41<00:00,  3.68img/s, loss (batch)=0.00683]\n",
      "INFO: Checkpoint 3 saved!\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.is_available() == True\n",
    "\n",
    "model = UNet()\n",
    "model = model.to(device=device, memory_format=torch.channels_last)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    epochs=3,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-8,\n",
    "    momentum=0.999,\n",
    "    gradient_clipping=1.0,\n",
    "    val_percent=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14a4179-f310-427a-94de-bfa019126edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_image(mask, mask_values):\n",
    "    width = mask.shape[-2]\n",
    "    height = mask.shape[-1]\n",
    "    number_of_classes = len(mask_values)\n",
    "\n",
    "    out = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # along channel axis\n",
    "    # return index c where value is True\n",
    "    mask = np.argmax(mask, axis=0) # for every pixel position, find the class of the pixel (that net classified it in)\n",
    "   \n",
    "    # for every class c\n",
    "    for c, mask_value in enumerate(mask_values):\n",
    "        # set pixel value in output to mask_value\n",
    "        # for every pixel that is located at position where predicted class == c\n",
    "        # for every pixel, mask contains indices of a class that net predicted given pixel is located in\n",
    "        out[mask == c] = mask_value\n",
    "        \n",
    "    return Image.fromarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05af7d89-5052-4a5d-86cc-f8dc4e42094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU:99.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc10793e080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: make this less ugly \n",
    "def predict(model, device, path_to_image, path_to_mask, out_thresh=0.5): \n",
    "    state_dict = torch.load(\"/home/jurica/Desktop/projekt/pokusaj/checkpoints/checkpoint_epoch3.pth\", map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    img_raw = Image.open(path_to_image)\n",
    "    img = np.asarray(img_raw.resize((572,572), resample=Image.Resampling.BICUBIC))\n",
    "    img_n = img.transpose((2, 0, 1))\n",
    "    img_n = img_n / 255.0\n",
    "    img_tensor = torch.tensor(img_n.copy()).float()\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    img_tensor = img_tensor.to(device=device)#, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    mask_raw_r = Image.open(path_to_mask)\n",
    "    mask_raw = np.asarray(mask_raw_r.resize((572,572), resample=Image.Resampling.BICUBIC).convert(\"RGB\")) \n",
    "    # create ground truth for cross entropy loss\n",
    "    masks_true = np.zeros((572, 572), dtype=np.int64)\n",
    "    mask_values = CarvanaImageMaskingChallangeDataset.get_mask_values()\n",
    "    for c, mask_class_pixel_value in enumerate(mask_values):\n",
    "        masks_true[(mask_raw == mask_class_pixel_value).all(-1)] = c\n",
    "\n",
    "    masks_true = torch.tensor(masks_true.copy()).long()\n",
    "    masks_true = masks_true.unsqueeze(0)\n",
    "    masks_true = masks_true.to(device=device)\n",
    "\n",
    "    mask = None\n",
    "    net_output = None\n",
    "\n",
    "    with torch.inference_mode(): # disable gradient calculation\n",
    "        model.eval()\n",
    "        net_output = model(img_tensor).cpu()\n",
    "        \n",
    "        # apply the sigmoid activation to get predictions in the range [0, 1]\n",
    "        # create new mask array, mask[i] = True if output probability > out_tresh \n",
    "        # if mask[i] = True then pixel[i] is in class c where c is channel where mask[i] = True\n",
    "        mask = torch.sigmoid(net_output) > out_thresh\n",
    "    \n",
    "        # .long() to convert True/False in 1/0\n",
    "        mask = mask[0].long().squeeze().numpy()\n",
    "        \n",
    "    \n",
    "    print(f\"IOU:{round(iou(net_output.to(device=device), masks_true)*100, 2)}%\")\n",
    "\n",
    "    return mask_raw, mask_to_image(mask, CarvanaImageMaskingChallangeDataset.get_mask_values())\n",
    "\n",
    "# test image from dataset not used during training\n",
    "mask_raw, mask_pred = predict(model,\n",
    "                                device, \n",
    "                                path_to_image=\"/home/jurica/Desktop/projekt/Pytorch-UNet/data/test_img/0cdf5b5d0ce1_04.jpg\",\n",
    "                                path_to_mask=\"/home/jurica/Desktop/projekt/Pytorch-UNet/data/test_mask/0cdf5b5d0ce1_04_mask.gif\"\n",
    "                               )\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(mask_raw)\n",
    "axarr[1].imshow(mask_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564a06a-0eab-4427-a692-cb20c2999624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
