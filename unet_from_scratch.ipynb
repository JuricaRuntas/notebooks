{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03028152-e80b-4477-bf9b-9d4dc7b9133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "\n",
    "USE_WANDB = False # enable wandb, TODO: read wandb docs and learn how to use it for real \n",
    "%env WANDB_SILENT=True\n",
    "WANDB_PROJECT_NAME = \"unet-from-scratch\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd8621f-bfca-4fe7-99e0-74ace30a141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b951d7-e017-4e24-8bcb-7baafb0b4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "# https://www.tutorialexample.com/fix-python-logging-module-not-writing-to-file-python-tutorial/\n",
    "for handler in logging.root.handlers:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s',\n",
    "                   handlers=[logging.FileHandler(\"logfile.log\", mode=\"a\"), stream_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff73e73-0f80-4384-aa4c-b4b9e86374d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarvanaImageMaskingChallangeDataset(Dataset):\n",
    "    PATH_TO_IMGS = \"/home/jurica/Desktop/projekt/Pytorch-UNet/data/imgs\"\n",
    "    PATH_TO_MASKS = \"/home/jurica/Desktop/projekt/Pytorch-UNet/data/masks\"\n",
    "    IMG_EXT = \".jpg\"\n",
    "    MASK_EXT = \"_mask.gif\"\n",
    "\n",
    "    # black: background\n",
    "    # white: car\n",
    "    mask_values = [[0,0,0], [255,255,255]]\n",
    "    \n",
    "    def __init__(self):\n",
    "        assert len(os.listdir(self.PATH_TO_IMGS)) == len(os.listdir(self.PATH_TO_MASKS))\n",
    "\n",
    "        # Each car has exactly 16 images, each one taken at different angles.\n",
    "        # Each car has a unique id and images are named according to id_01.jpg, id_02.jpg … id_16.jpg.\n",
    "        self.img_ids = [os.path.splitext(img)[0] for img in os.listdir(self.PATH_TO_IMGS)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_ids[idx] + self.IMG_EXT\n",
    "        img_path = os.path.join(self.PATH_TO_IMGS, img_name)\n",
    "\n",
    "        mask_name = self.img_ids[idx] + self.MASK_EXT\n",
    "        mask_path = os.path.join(self.PATH_TO_MASKS, mask_name)\n",
    "        \n",
    "        img_raw = Image.open(img_path)\n",
    "        mask_raw = Image.open(mask_path)\n",
    "        \n",
    "        img, mask = self.preprocess(img_raw, mask_raw)\n",
    "        \n",
    "        return {\"image\": img, \"mask\": mask}\n",
    "\n",
    "    @classmethod\n",
    "    def get_mask_values(cls):\n",
    "        return cls.mask_values\n",
    "\n",
    "    def preprocess(self, img, mask):\n",
    "        img = np.asarray(img.resize((572,572), resample=Image.Resampling.BICUBIC))\n",
    "        # convert to RGB for black&white mask\n",
    "        mask = np.asarray(mask.resize((572,572), resample=Image.Resampling.BICUBIC).convert(\"RGB\")) \n",
    "        \n",
    "        # create ground truth for cross entropy loss\n",
    "        mask_with_classes = np.zeros((572, 572), dtype=np.int64)\n",
    "        for c, mask_class_pixel_value in enumerate(self.mask_values):\n",
    "            # (img == mask_class_pixel_value) creates new np array, (img==mask_class_pixel_value).shape == img.shape\n",
    "            # (img==mask_class_pixel_value)[i] -> [True, False, True], \n",
    "            # (comparing corresponding pixel components with corresponding mask_class_pixel_value components)\n",
    "            # (img==mask_class_pixel_value).all(-1) creates new np array\n",
    "            # x.all(-1) checks if every element of x along -1 (last) dimension is True\n",
    "            # (img==mask_class_pixel_value).all(-1).shape == (H, W)\n",
    "            # in this case, check which (img==mask_class_pixel_value)[i] is equal to [True, True, True] (pixel matches class mask_class_pixel_value)\n",
    "            \n",
    "            # set pixel class to be c for every pixel in original mask image\n",
    "            # if pixel values along all channels (-1) correspondingly match values of class c\n",
    "            mask_with_classes[(mask == mask_class_pixel_value).all(-1)] = c\n",
    "        \n",
    "        # swap color axis\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        # normalize pixels?\n",
    "        img = img / 255.0\n",
    "\n",
    "        img_tensor = torch.tensor(img.copy()).to(dtype=torch.float32)\n",
    "        \n",
    "        mask_tensor = torch.tensor(mask_with_classes.copy()).to(dtype=torch.long) # Cross entropy loss requires target to be LongTensor\n",
    "        \n",
    "        return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1767549-07ea-4598-90d0-bd22a2ca4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_volume_channels, output_volume_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(input_volume_channels, output_volume_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(output_volume_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(output_volume_channels, output_volume_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(output_volume_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_volume_channels, output_volume_channels):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(input_volume_channels, output_volume_channels, kernel_size=2, stride=2)\n",
    "        self.double_conv = EncoderBlock(input_volume_channels, output_volume_channels)\n",
    "    \n",
    "    def forward(self, x, encoder_feature_map):\n",
    "        deconv_ret = self.deconv(x)\n",
    "\n",
    "        diffInHeight = encoder_feature_map.size()[2] - deconv_ret.size()[2]\n",
    "        diffInWidth = encoder_feature_map.size()[3] - deconv_ret.size()[3]\n",
    "        \n",
    "        # [padding_left, padding_right, padding_top, padding_bottom]\n",
    "        # now deconv_ret.shape == encoder_feature_map.shape\n",
    "        deconv_ret = nn.functional.pad(deconv_ret, [diffInWidth // 2, diffInWidth - diffInWidth // 2,\n",
    "                        diffInHeight // 2, diffInHeight - diffInHeight // 2])\n",
    "\n",
    "        # concat along \"channel\" dimension, dim=1 because dim=0 -> N (batch size) \n",
    "        concat_x = torch.cat([encoder_feature_map, deconv_ret], dim=1)\n",
    "\n",
    "        return self.double_conv(concat_x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc_block1 = EncoderBlock(3, 64)      \n",
    "        self.enc_block2 = EncoderBlock(64, 128)\n",
    "        self.enc_block3 = EncoderBlock(128, 256)\n",
    "        self.enc_block4 = EncoderBlock(256, 512)\n",
    "        self.enc_block5 = EncoderBlock(512, 1024)\n",
    "\n",
    "        self.dec_block1 = DecoderBlock(1024, 512)\n",
    "        self.dec_block2 = DecoderBlock(512, 256)\n",
    "        self.dec_block3 = DecoderBlock(256, 128)\n",
    "        self.dec_block4 = DecoderBlock(128, 64)\n",
    "\n",
    "        self.final_output = nn.Conv2d(64, 2, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        enc_out1 = self.enc_block1(x)\n",
    "        pooled_enc_out1 = self.max_pool(enc_out1)\n",
    "        \n",
    "        enc_out2 = self.enc_block2(pooled_enc_out1)\n",
    "        pooled_enc_out2 = self.max_pool(enc_out2)\n",
    "        \n",
    "        enc_out3 = self.enc_block3(pooled_enc_out2)\n",
    "        pooled_enc_out3 = self.max_pool(enc_out3)\n",
    "        \n",
    "        enc_out4 = self.enc_block4(pooled_enc_out3)\n",
    "        pooled_enc_out4 = self.max_pool(enc_out4)\n",
    "        \n",
    "        enc_out5 = self.enc_block5(pooled_enc_out4)\n",
    "\n",
    "        dec_out1 = self.dec_block1(enc_out5, enc_out4)\n",
    "        dec_out2 = self.dec_block2(dec_out1, enc_out3)\n",
    "        dec_out3 = self.dec_block3(dec_out2, enc_out2)\n",
    "        dec_out4 = self.dec_block4(dec_out3, enc_out1)\n",
    "\n",
    "        return nn.functional.interpolate(self.final_output(dec_out4), size=572, mode=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174a386f-a76e-46e7-a760-9e23d7efb987",
   "metadata": {},
   "outputs": [],
   "source": [
    " def iou(masks_pred, masks):\n",
    "    # resulting tensor of shape (N, masks_predH, masks_predW)\n",
    "    # each element of resulting tensor is index of value that is max at given position across channel dimension\n",
    "    # e.g., if we have 2 channels (2 classes) then values of resulting tensor are in range [0,1]\n",
    "    most_certain_pred_values = masks_pred.argmax(dim=1)\n",
    "    \n",
    "    # one_hot appends number of classes as last dimension\n",
    "    # tensor of shape (N, H, W) becomes (N, H, W, n_classes), have to correct it to (N, C, H, W)\n",
    "    masks_pred = nn.functional.one_hot(most_certain_pred_values, len(CarvanaImageMaskingChallangeDataset.get_mask_values())).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    masks = nn.functional.one_hot(masks, len(CarvanaImageMaskingChallangeDataset.get_mask_values())).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "    _iou = MulticlassJaccardIndex(num_classes=2, average=\"macro\").to(device=device) # Calculate statistics for each label and average them\n",
    "    return _iou(masks_pred, masks).item()\n",
    "\n",
    "def train_model(model, device, epochs, batch_size, learning_rate, weight_decay, momentum, gradient_clipping, val_percent):\n",
    "    dataset = CarvanaImageMaskingChallangeDataset()\n",
    "\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], torch.Generator().manual_seed(1337))\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=False, drop_last=True)\n",
    "\n",
    "    #optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, foreach=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay, fused=True)\n",
    "    learning_rate = 0.001 # default Adam learning rate\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5) # maximize IOU\n",
    "    criterion = nn.CrossEntropyLoss() # default reduction: the weighted mean of the output of LogSoftmax is taken\n",
    "\n",
    "\n",
    "    if USE_WANDB:\n",
    "        experiment = wandb.init(project=WANDB_PROJECT_NAME)\n",
    "        experiment.config.update(\n",
    "        dict(epochs=epochs, \n",
    "             batch_size=batch_size,\n",
    "             \n",
    "             n_train=n_train,\n",
    "             n_val=n_val,\n",
    "             \n",
    "             learning_rate=learning_rate,\n",
    "             weight_decay=weight_decay,\n",
    "             momentum=momentum,\n",
    "             gradient_clipping=gradient_clipping,\n",
    "             \n",
    "             val_percent=val_percent)\n",
    "        )\n",
    "\n",
    "    logging.info(\n",
    "        f\"\"\"Starting training:\n",
    "        Epochs:              {epochs}\n",
    "        Batch size:          {batch_size}\n",
    "\n",
    "        Training set size:   {n_train}\n",
    "        Validation set size: {n_val}\n",
    "        \n",
    "        Learning rate:       {learning_rate}\n",
    "        Weight decay:        {weight_decay}\n",
    "        Momentum:            {momentum}\n",
    "        Gradient clipping:   {gradient_clipping}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # During training, the global_step is updated every time a batch is processed\n",
    "    # When logging training metrics to wandb, the global_step is used as the x-axis to indicate this\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train() # for BatchNorm\n",
    "\n",
    "        with tqdm(total=n_train, desc=f\"Epoch {epoch}/{epochs}\", unit=\"img\") as progress:\n",
    "            for batch in train_loader:\n",
    "                images = batch[\"image\"].to(device=device, memory_format=torch.channels_last)\n",
    "                masks = batch[\"mask\"].to(device=device)\n",
    "\n",
    "                masks_pred = model(images)\n",
    "\n",
    "                loss = criterion(masks_pred, masks)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping) # can't hurt?\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                # update number of remaining images\n",
    "                progress.update(images.shape[0])\n",
    "                progress.set_postfix(**{\"loss (batch)\" : loss.item()})\n",
    "                global_step += 1\n",
    "\n",
    "                if USE_WANDB:\n",
    "                    experiment.log({\n",
    "                    \"train loss\": loss.item(),\n",
    "                    \"step\": global_step,\n",
    "                    \"epoch\": epoch\n",
    "                    })\n",
    "                \n",
    "                division_step = (n_train // (5 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        if USE_WANDB:\n",
    "                            histograms = {}\n",
    "                            for tag, value in model.named_parameters():\n",
    "                                tag = tag.replace('/', '.')\n",
    "                                if not (torch.isinf(value) | torch.isnan(value)).any():\n",
    "                                    histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                                if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
    "                                    histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "                        \n",
    "                        val_score = 0\n",
    "                        with torch.inference_mode(): # disable gradient calculation and other vudu\n",
    "                            model.eval() # for BatchNorm\n",
    "                            \n",
    "                            iou_score = 0\n",
    "    \n",
    "                            # leave=True? Position=0?\n",
    "                            for batch in tqdm(val_loader, total=n_val, desc=\"Validation\", unit=\"batch\", position=0, leave=True):\n",
    "                                images = batch[\"image\"].to(device=device, memory_format=torch.channels_last)\n",
    "                                masks = batch[\"mask\"].to(device=device)\n",
    "    \n",
    "                                masks_pred = model(images)\n",
    "\n",
    "                                iou_score += iou(masks_pred, masks)\n",
    "                            \n",
    "                            model.train()\n",
    "                            val_score = iou_score / max(n_val, 1)\n",
    "                            logging.info(f\"IOU score: {val_score}\")\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        if USE_WANDB:\n",
    "                            experiment.log({\n",
    "                                'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                                'validation IOU': iou_score,\n",
    "                                'step': global_step,\n",
    "                                'epoch': epoch,\n",
    "                                **histograms\n",
    "                            })                     \n",
    "        \n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, \"/home/jurica/Desktop/projekt/pokusaj/checkpoints/checkpoint_epoch{}.pth\".format(epoch))\n",
    "        logging.info(f\"Checkpoint {epoch} saved!\")\n",
    "\n",
    "    if USE_WANDB:\n",
    "        experiment.finish()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9068110-a928-4caf-9f00-54a692289d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting training:\n",
      "       Epochs:              3\n",
      "       Batch size:          1\n",
      "\n",
      "       Training set size:   4565\n",
      "       Validation set size: 507\n",
      "       \n",
      "       Learning rate:       0.001\n",
      "       Weight decay:        1e-08\n",
      "       Momentum:            0.999\n",
      "       Gradient clipping:   1.0\n",
      "       \n",
      "Validation: 100%|██████████████████████████| 507/507 [00:39<00:00, 12.81batch/s]\n",
      "INFO: IOU score: 0.8932622394853325\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:36<00:00, 13.82batch/s]\n",
      "INFO: IOU score: 0.9797208322105558\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:36<00:00, 13.85batch/s]\n",
      "INFO: IOU score: 0.9259961474341518\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:39<00:00, 12.99batch/s]\n",
      "INFO: IOU score: 0.9039142315204327\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:38<00:00, 13.29batch/s]\n",
      "INFO: IOU score: 0.9328345683434541\n",
      "Epoch 1/3: 100%|██████| 4565/4565 [20:56<00:00,  3.63img/s, loss (batch)=0.0158]\n",
      "INFO: Checkpoint 1 saved!\n",
      "Validation: 100%|██████████████████████████| 507/507 [00:37<00:00, 13.65batch/s]\n",
      "INFO: IOU score: 0.9824855978907447\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:36<00:00, 13.72batch/s]\n",
      "INFO: IOU score: 0.9675655434352641\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:36<00:00, 13.72batch/s]\n",
      "INFO: IOU score: 0.9846765427194404\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.69batch/s]\n",
      "INFO: IOU score: 0.984467105282364\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.67batch/s]\n",
      "INFO: IOU score: 0.9873465162057143\n",
      "Epoch 2/3: 100%|█████| 4565/4565 [20:38<00:00,  3.69img/s, loss (batch)=0.00845]\n",
      "INFO: Checkpoint 2 saved!\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.66batch/s]\n",
      "INFO: IOU score: 0.9787254426371181\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.69batch/s]\n",
      "INFO: IOU score: 0.9815683022758664\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.69batch/s]\n",
      "INFO: IOU score: 0.978822976409566\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:37<00:00, 13.49batch/s]\n",
      "INFO: IOU score: 0.9396538723855329\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [00:36<00:00, 13.71batch/s]\n",
      "INFO: IOU score: 0.9820751122233901\n",
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4565/4565 [20:41<00:00,  3.68img/s, loss (batch)=0.00683]\n",
      "INFO: Checkpoint 3 saved!\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.is_available() == True\n",
    "\n",
    "model = UNet()\n",
    "model = model.to(device=device, memory_format=torch.channels_last)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    epochs=3,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-8,\n",
    "    momentum=0.999,\n",
    "    gradient_clipping=1.0,\n",
    "    val_percent=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14a4179-f310-427a-94de-bfa019126edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_image(mask, mask_values):\n",
    "    width = mask.shape[-2]\n",
    "    height = mask.shape[-1]\n",
    "    number_of_classes = len(mask_values)\n",
    "\n",
    "    out = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # along channel axis\n",
    "    # return index c where value is True\n",
    "    mask = np.argmax(mask, axis=0) # for every pixel position, find the class of the pixel (that net classified it in)\n",
    "   \n",
    "    # for every class c\n",
    "    for c, mask_value in enumerate(mask_values):\n",
    "        # set pixel value in output to mask_value\n",
    "        # for every pixel that is located at position where predicted class == c\n",
    "        # for every pixel, mask contains indices of a class that net predicted given pixel is located in\n",
    "        out[mask == c] = mask_value\n",
    "        \n",
    "    return Image.fromarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05af7d89-5052-4a5d-86cc-f8dc4e42094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU:99.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f08ec044d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: make this less ugly \n",
    "def predict(model, device, path_to_image, path_to_mask, out_thresh=0.5): \n",
    "    state_dict = torch.load(\"/home/jurica/Desktop/projekt/pokusaj/checkpoints/checkpoint_epoch3.pth\", map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    img_raw = Image.open(path_to_image)\n",
    "    img = np.asarray(img_raw.resize((572,572), resample=Image.Resampling.BICUBIC))\n",
    "    img_n = img.transpose((2, 0, 1))\n",
    "    img_n = img_n / 255.0\n",
    "    img_tensor = torch.tensor(img_n.copy()).float()\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    img_tensor = img_tensor.to(device=device)#, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    mask_raw_r = Image.open(path_to_mask)\n",
    "    mask_raw = np.asarray(mask_raw_r.resize((572,572), resample=Image.Resampling.BICUBIC).convert(\"RGB\")) \n",
    "    # create ground truth for cross entropy loss\n",
    "    masks_true = np.zeros((572, 572), dtype=np.int64)\n",
    "    mask_values = CarvanaImageMaskingChallangeDataset.get_mask_values()\n",
    "    for c, mask_class_pixel_value in enumerate(mask_values):\n",
    "        masks_true[(mask_raw == mask_class_pixel_value).all(-1)] = c\n",
    "\n",
    "    masks_true = torch.tensor(masks_true.copy()).long()\n",
    "    masks_true = masks_true.unsqueeze(0)\n",
    "    masks_true = masks_true.to(device=device)\n",
    "\n",
    "    mask = None\n",
    "    net_output = None\n",
    "\n",
    "    with torch.inference_mode(): # disable gradient calculation\n",
    "        model.eval()\n",
    "        net_output = model(img_tensor).cpu()\n",
    "        \n",
    "        # apply the sigmoid activation to get predictions in the range [0, 1]\n",
    "        # create new mask array, mask[i] = True if output probability > out_tresh \n",
    "        # if mask[i] = True then pixel[i] is in class c where c is channel where mask[i] = True\n",
    "        mask = torch.sigmoid(net_output) > out_thresh\n",
    "    \n",
    "        # .long() to convert True/False in 1/0\n",
    "        mask = mask[0].long().squeeze().numpy()\n",
    "        \n",
    "    \n",
    "    print(f\"IOU:{round(iou(net_output.to(device=device), masks_true)*100, 2)}%\")\n",
    "\n",
    "    return mask_raw, mask_to_image(mask, CarvanaImageMaskingChallangeDataset.get_mask_values())\n",
    "\n",
    "mask_raw, mask_pred = predict(model, \n",
    "                                device, \n",
    "                                path_to_image=\"/home/jurica/Desktop/projekt/Pytorch-UNet/data/test_img/0cdf5b5d0ce1_04.jpg\",\n",
    "                                path_to_mask=\"/home/jurica/Desktop/projekt/Pytorch-UNet/data/test_mask/0cdf5b5d0ce1_04_mask.gif\"\n",
    "                               )\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "axarr[0].imshow(mask_raw)\n",
    "axarr[1].imshow(mask_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2449859d-a8b6-4c70-88ed-61156ea3adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_mask(img, mask):\n",
    "    classes = mask.max() + 1\n",
    "    fig, ax = plt.subplots(1, classes + 1)\n",
    "    ax[0].set_title('Input image')\n",
    "    ax[0].imshow(img)\n",
    "    for i in range(classes):\n",
    "        ax[i + 1].set_title(f'Mask (class {i + 1})')\n",
    "        ax[i + 1].imshow(mask == i)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564a06a-0eab-4427-a692-cb20c2999624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
